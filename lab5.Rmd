---
title: "Lab 5"
output: github_document
---
### Econ B2000, MA Econometrics
### Kevin R Foster, the Colin Powell School at the City College of New York, CUNY
### Fall 2023

For this lab, we improve some of our regression models to explain wages.

*Note that since exam is next week, this lab will not turn into homework.*

Work with a group to prepare a 5-min presentation by one of the group members about experiment process and results. You get 75 min to prepare.

Build on the previous lab in creating useful models.

### Note on coding
First a note to advance your coding. The following bits of code do the same thing:
```{r eval=FALSE}
attach(acs2017_ny)
model_v1 <- lm(INCWAGE ~ AGE)
detach()

model_v2 <- lm(acs2017_ny$INCWAGE ~ acs2017_ny$AGE)

model_v3 <- lm(INCWAGE ~ AGE, data = acs2017_ny)

```
I prefer the last one, v3. I think v2 is too verbose (especially once you have dozens of variables in your model) while v1 is liable to errors since, if the `attach` command gets too far separated from the `lm()` within your code chunks, can have unintended consequences. Later you will be doing robustness checks, where you do the same regression on slightly different subsets. (For example, compare a model fit on all people 18-65 vs people 25-55 vs other age ranges.) In that case v3 becomes less verbose as well as less liable to have mistakes.

However specifying the dataset, as with v3, means that any preliminary data transformations should modify the original dataset. So if you create `newvarb <- f(oldvarb)`, you have to carefully set `dataset$newvarb <- f(dataset$oldvarb)` and think about which dataset gets that transformation. The coding forces you to think about which dataset gets the transformation, which is good.

While we've used 'attach' and 'detach' previously (and, whew boy, everybody sometimes has trouble with cleaning up *detach* after a bunch of *attach* commands!) I think it's  time to take off the training wheels and rampdown your use of 'attach' and 'detach'.

### Back to Lab
Concentrate on a smaller subset than previous. For instance if you wanted to look at wages for Hispanic women with at least a college degree, you might use

```{r eval = FALSE}
attach(acs2017_ny)
use_varb <- (AGE >= 25) & (AGE <= 55) & (LABFORCE == 2) & (WKSWORK2 > 4) & (UHRSWORK >= 35) & (Hispanic == 1) & (female == 1) & ((educ_college == 1) | (educ_advdeg == 1))
dat_use <- subset(acs2017_ny,use_varb) # 
detach()
```

*Although, really, you should make sure you know what each one of those restrictions is doing -- recall the first rule of analysis, "Know your data". Check the codebook (in the zip or also in this repo for your convenience). Some simple summary stats would be good here.*

Your group should pick a different subset.
  
Try a regression with age and age-squared in addition to your other controls, something like this:

```{r eval=FALSE}
lm((INCWAGE ~ Age + I(Age^2) + ... ) )

```

What is the peak of predicted wage? What if you add higher order polynomials of age, such as $Age^3$ or $Age^4$? Do a hypothesis test of whether all of those higher-order polynomial terms are *jointly* significant. Describe the pattern of predicted wage as a function of age. What if you used $log(Age)$? (And why would polynomials in $log(Age)$ be useless? Experiment.)

Recall about how dummy variables work. If you added educ_hs in a regression using the subset given above, what would that do? (Experiment, if you aren't sure.) What is interpretation of coefficient on *educ_college* in that subset? What would happen if you put both *educ_college* and *educ_advdeg* into a regression? Are your other dummy variables in the regression working sensibly with your selection criteria?

Why don't we use polynomial terms of dummy variables? Experiment.

What is the predicted wage, from your model, for a few relevant cases? Do those seem reasonable?

What is difference in regression from using log wage as the dependent variable? Compare the pattern of predicted values from the two models (remember to take exp() of the predicted value, where the dependent is log wage). Discuss.

Try some interactions, like this,
```{r eval=FALSE}
lm(INCWAGE ~ Age + I(Age^2) + female + I(female*Age) + I(female*(Age^2) + ... ) 
```
and explain those outputs (different peaks for different groups).

What are the other variables you are using in your regression? Do they have the expected signs and patterns of significance? Explain if there is a plausible causal link from X variables to Y and not the reverse. Explain your results, giving details about the estimation, some predicted values, and providing any relevant graphics. Impress.

